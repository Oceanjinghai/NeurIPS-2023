import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, Dataset
import random 
from matplotlib import pyplot as plt
class MetaDataset(Dataset):
    def __init__(self, data, y):
        self.data = torch.tensor(data, dtype=torch.float32)
        self.y = torch.tensor(y, dtype=torch.float32)

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], self.y[idx]
class FeatureExtractor(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim):
        super(FeatureExtractor, self).__init__()
        self.network = nn.Sequential(
            nn.Linear(input_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, output_dim),
            nn.ReLU()
        )

    def forward(self, x):
        return self.network(x)
    
class RegressionModel(nn.Module):
    def __init__(self, input_dim):
        super(RegressionModel, self).__init__()
        self.linear = nn.Linear(input_dim, 1)

    def forward(self, x):
        return self.linear(x)
    
def maml(feature_extractor, meta_datasets, inner_lr, outer_lr, num_inner_updates, num_outer_updates):
    outer_optimizer = optim.Adam(feature_extractor.parameters(), lr=outer_lr)

    for outer_step in range(num_outer_updates):
        outer_loss = 0.0

        for task in meta_datasets:
            task_data, task_y = task
            task_dataset = MetaDataset(task_data, task_y)
            task_dataloader = DataLoader(task_dataset, batch_size=1)

            inner_model = RegressionModel(reduced_dim)
            inner_optimizer = optim.SGD(inner_model.parameters(), lr=inner_lr)

            # Inner loop
            for inner_step in range(num_inner_updates):
                for batch_data, batch_y in task_dataloader:
                    batch_data = batch_data.view(-1, original_dim)
                    embedded_data = feature_extractor(batch_data)
                    y_pred = inner_model(embedded_data).squeeze()
                    inner_loss = nn.MSELoss()(y_pred.view(-1, 1), batch_y.view(-1, 1))
                    inner_optimizer.zero_grad()
                    inner_loss.backward()
                    inner_optimizer.step()

            # Outer loop
            for batch_data, batch_y in task_dataloader:
                batch_data = batch_data.view(-1, original_dim)
                embedded_data = feature_extractor(batch_data)
                y_pred = inner_model(embedded_data).squeeze()
                task_outer_loss = nn.MSELoss()(y_pred.view(-1, 1), batch_y.view(-1, 1))
                outer_loss += task_outer_loss

        outer_loss /= len(meta_datasets)
        outer_optimizer.zero_grad()
        outer_loss.backward()
        outer_optimizer.step()

        print(f"Outer step {outer_step + 1}/{num_outer_updates}, outer loss: {outer_loss.item()}")
    return inner_model
def evaluate_model(feature_extractor, inner_model, eval_datasets, num_shots_list, inner_lr, num_inner_updates):
    avg_test_losses = []
    for num_shots in num_shots_list:
        test_losses = []
        for task_idx, (task_data,_, task_y) in enumerate(eval_datasets):
            # Convert data to tensors
            task_data = torch.tensor(task_data, dtype=torch.float32)
            task_y = torch.tensor(task_y, dtype=torch.float32)

            # Sample a few-shot dataset for the task
            eval_dataset_size = len(task_data)
            shot_indices = random.sample(range(eval_dataset_size), num_shots)
            shot_data = task_data[shot_indices]
            shot_y = task_y[shot_indices]

            # Train the inner model on the few-shot dataset
            inner_optimizer = torch.optim.Adam(inner_model.parameters(), lr=inner_lr)
            for _ in range(num_inner_updates):
                embedded_data = feature_extractor(shot_data)
                y_pred = inner_model(embedded_data).squeeze()
                inner_loss = nn.MSELoss()(y_pred, shot_y)
                inner_optimizer.zero_grad()
                inner_loss.backward()
                inner_optimizer.step()

            # Evaluate the inner model on the task dataset
            embedded_data = feature_extractor(task_data)
            y_pred = inner_model(embedded_data).squeeze()
            test_loss = nn.MSELoss()(y_pred, task_y)
            test_losses.append(test_loss.item())

        avg_test_loss = sum(test_losses) / len(test_losses)
        avg_test_losses.append(avg_test_loss)
        print(f"Number of Shots: {num_shots}, Average Test Loss: {avg_test_loss}")

    # Plot the average test losses
    plt.plot(num_shots_list, avg_test_losses)
    plt.xlabel("Number of Shots")
    plt.ylabel("Average Test Loss")
    plt.title("Average Test Losses for Different Numbers of Shots")
    plt.show()
